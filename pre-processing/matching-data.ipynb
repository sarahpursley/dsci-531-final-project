{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0919a3e3-a7db-4862-ac9c-771d33c69d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from sklearn.decomposition import PCA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6633010-d9f6-4ece-8783-d8dec03a752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.getcwd(),\"tweet_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8170d6f7-c8e8-4233-9309-451815e69c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtag_usage_frequency(joined_data):\n",
    "    hashtags = []\n",
    "    a = joined_data[\"lc_hashtags\"].tolist()\n",
    "    for h in range(len(a)):\n",
    "        if(a[h]):\n",
    "            for i in a[h]:\n",
    "                hashtags.append(i)\n",
    "    hashtags = [x.lower() for x in hashtags]\n",
    "    hashtag_frequency = collections.Counter(hashtags)\n",
    "    hashtag_frequency = hashtag_frequency.most_common()\n",
    "    return(hashtag_frequency)\n",
    "\n",
    "def get_match(a,control,w):\n",
    "\n",
    "    if(w % 50 == 0):\n",
    "        print(\"We have completed \", w, \" matchings\")\n",
    "    cosin_sims = [np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)) for b in control]\n",
    "    control_index = cosin_sims.index(max(cosin_sims))\n",
    "    \n",
    "    return(control_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa28d715-3a14-4084-baea-0eaef80781e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = pd.read_csv(os.path.join(datapath,\"treated_df.csv\"))\n",
    "control = pd.read_csv(os.path.join(datapath,\"control_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1817a2b4-6062-4d3a-8a40-132f01040370",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated[\"lc_hashtags\"] = [np.matrix(w).A[0].tolist() for w in treated[\"lc_hashtags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83c13173-d92b-47d9-9dd9-32c1a31cbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_hashs = get_hashtag_usage_frequency(treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cbf8b5b7-f0df-44c7-9e3c-3d04ca21780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = np.percentile([j[1] for j in used_hashs],[95.5,96])[0]\n",
    "high = np.percentile([j[1] for j in used_hashs],[95.5,96])[1]\n",
    "relevant_hashs = [h[0] for h in used_hashs if h[1] >= low and h[1] <= high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e0f79d2-4418-4105-948c-e79e56f483ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_treatment_observations = [h for h in range(len(treated)) if \n",
    " any(j in relevant_hashs for j in treated.iloc[h].lc_hashtags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d7fd5a38-74f9-4d73-a787-4892ec4b7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = treated.iloc[kept_treatment_observations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b2aa576-f951-4940-87aa-2303b9bfe8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated.word_vectors = [np.matrix(w).A[0].tolist() for w in treated.word_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62867264-9138-4df8-a6d7-54d4f947c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control.word_vectors = [np.matrix(w).A[0].tolist() for w in control.word_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "32d09721-ce70-42c1-810a-d6eaf0217458",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(treated.reset_index().word_vectors.tolist())\n",
    "b = pd.DataFrame(control.reset_index().word_vectors.tolist())\n",
    "pca = PCA(n_components=10)\n",
    "x = pd.concat([a, b], ignore_index=True)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "a = principalDf.iloc[0:len(treated)].values.tolist()\n",
    "b = principalDf.iloc[len(treated):].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e30b15fa-e281-428d-bc5c-8d772235fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated[\"pca_reduced_vectors\"] = a\n",
    "control[\"pca_reduced_vectors\"] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "297fed84-1051-4c89-a485-93f6a145f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have completed  0  matchings\n",
      "We have completed  50  matchings\n",
      "We have completed  100  matchings\n",
      "We have completed  150  matchings\n",
      "We have completed  200  matchings\n",
      "We have completed  250  matchings\n",
      "We have completed  300  matchings\n",
      "We have completed  350  matchings\n",
      "We have completed  400  matchings\n",
      "We have completed  450  matchings\n",
      "We have completed  500  matchings\n",
      "We have completed  550  matchings\n",
      "We have completed  600  matchings\n",
      "We have completed  650  matchings\n",
      "We have completed  700  matchings\n",
      "We have completed  750  matchings\n",
      "We have completed  800  matchings\n",
      "We have completed  850  matchings\n",
      "We have completed  900  matchings\n",
      "We have completed  950  matchings\n",
      "We have completed  1000  matchings\n",
      "We have completed  1050  matchings\n",
      "We have completed  1100  matchings\n",
      "We have completed  1150  matchings\n",
      "We have completed  1200  matchings\n",
      "We have completed  1250  matchings\n",
      "We have completed  1300  matchings\n",
      "We have completed  1350  matchings\n",
      "We have completed  1400  matchings\n",
      "We have completed  1450  matchings\n",
      "We have completed  1500  matchings\n",
      "We have completed  1550  matchings\n",
      "We have completed  1600  matchings\n",
      "We have completed  1650  matchings\n",
      "We have completed  1700  matchings\n",
      "We have completed  1750  matchings\n",
      "We have completed  1800  matchings\n",
      "We have completed  1850  matchings\n",
      "We have completed  1900  matchings\n",
      "We have completed  1950  matchings\n",
      "We have completed  2000  matchings\n",
      "We have completed  2050  matchings\n",
      "We have completed  2100  matchings\n",
      "We have completed  2150  matchings\n",
      "We have completed  2200  matchings\n",
      "We have completed  2250  matchings\n",
      "We have completed  2300  matchings\n",
      "We have completed  2350  matchings\n",
      "We have completed  2400  matchings\n",
      "We have completed  2450  matchings\n",
      "We have completed  2500  matchings\n",
      "We have completed  2550  matchings\n",
      "We have completed  2600  matchings\n",
      "We have completed  2650  matchings\n",
      "We have completed  2700  matchings\n",
      "We have completed  2750  matchings\n",
      "We have completed  2800  matchings\n",
      "We have completed  2850  matchings\n",
      "We have completed  2900  matchings\n",
      "We have completed  2950  matchings\n",
      "We have completed  3000  matchings\n",
      "We have completed  3050  matchings\n",
      "We have completed  3100  matchings\n",
      "We have completed  3150  matchings\n",
      "We have completed  3200  matchings\n",
      "We have completed  3250  matchings\n",
      "We have completed  3300  matchings\n",
      "We have completed  3350  matchings\n",
      "We have completed  3400  matchings\n",
      "We have completed  3450  matchings\n",
      "We have completed  3500  matchings\n",
      "We have completed  3550  matchings\n",
      "This took  16008.630291223526  seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "control_vectors = [get_match(treated.iloc[w][\"pca_reduced_vectors\"],\n",
    "                             control[\"pca_reduced_vectors\"],w) for w in range(len(treated))]\n",
    "end = time.time()\n",
    "print(\"This took \", end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0b87f4a6-d917-4492-a5d0-9c65c15411e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated[\"control_match\"] = control_vectors\n",
    "c = pd.DataFrame(control_vectors)\n",
    "c.to_csv(os.path.join(datapath,\"matched_control_vectors_df.csv\"))\n",
    "matched_controls = control.iloc[control_vectors]\n",
    "matched_controls[\"lc_hashtags\"] = [np.matrix(w).A[0].tolist() for\n",
    "                                w in matched_controls[\"lc_hashtags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eab5d655-9f1b-4ff8-9d19-418da4e7ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_matched_unique_hashtags = get_hashtag_usage_frequency(matched_controls)\n",
    "treated_unique_hashtags = get_hashtag_usage_frequency(treated)\n",
    "treated_unique_hashtags = pd.DataFrame(treated_unique_hashtags,columns = [\"hashtag\",\"count\"])\n",
    "control_matched_unique_hashtags = pd.DataFrame(control_matched_unique_hashtags,\n",
    "                                               columns = [\"hashtag\",\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4eabc681-3978-499c-8e53-a91953b44a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2263"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(control_matched_unique_hashtags))\n",
    "len(treated_unique_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "038c0182-53fc-4053-9f29-132b81b714c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treated_unique_hashtags.hashtag.tolist()\n",
    "control_matched_unique_hashtags.hashtag.tolist()\n",
    "len(list(set(treated_unique_hashtags.hashtag.tolist()) -\n",
    "     set(control_matched_unique_hashtags.hashtag.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cf8730bc-5ffe-4878-ae3b-369d941cdce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-226-5b7711ed8bb3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matched_controls[\"user_id\"] = [ast.literal_eval(u).get(\"id\") for u in matched_controls.user]\n"
     ]
    }
   ],
   "source": [
    "treated[\"user_id\"] = [ast.literal_eval(u).get(\"id\") for u in treated.user]\n",
    "matched_controls[\"user_id\"] = [ast.literal_eval(u).get(\"id\") for u in matched_controls.user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a85cf8d0-25fa-462d-a015-d81e5ec02ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "treated.to_csv(os.path.join(datapath,\"tweets\",\"DAPL\",\"final_treated_df.csv\"))\n",
    "matched_controls.to_csv(os.path.join(datapath,\"tweets\",\"DAPL\",\"final_control_df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "deb2008d-a681-4e95-81cb-4fed5634277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Poet\\\\usc_coursework\\\\spring-2022\\\\fairness-in-ai\\\\project\\\\tweet_data\\\\tweets\\\\DAPL\\\\final_control_df.csv'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(datapath,\"tweets\",\"DAPL\",\"final_control_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cf4206e2-f1d5-4944-8473-85553b04f088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17184930"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "test_string = treated.user.iloc[0]\n",
    "#json.loads()\n",
    "ast.literal_eval(test_string).get(\"id\")\n",
    "#.get(\"id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
